# A Survey of Large Language Models

Created by: yj z
Created time: May 30, 2023 4:33 PM
Last edited by: Azyka
Last edited time: June 2, 2023 11:32 AM

# 背景

通常，大型语言模型（LLM）是指包含数千亿（或更多）参数的Transformer语言模型，这些参数是在大量无标签文本数据上训练的。LLM表现出很强的理解自然语言和解决复杂任务的能力。

## Scaling Law

现有的LLM模型采用的Transformer结构和预训练目标与小型的语言模型差别不大。但LLM极大地扩展了模型大小、数据规模和总计算量。在计算资源有限的情况下，提前预知不同参数选择对模型性能的影响非常重要。

- **KM scaling law.** Open AI 团队在2020年首次提出将模型关系与三大要素进行建模：模型大小 ($N$)、数据集大小 ($D$)、训练计算量 ($C$)。其单位分别为模型参数数量，训练集token数量，FP天数（petaflops/s-day，标准算力单位，每秒钟进行10的15次方运算运行1天）。给定计算预算 $c$，他们提出的基本定理如下：

$$
L(N)=(\frac{N_c}{N})^{\alpha_N}, \alpha_N\sim0.076,N_c\sim8.8\times10^{13}
$$

$$
L(D)=(\frac{D_c}{D})^{\alpha_D}, \alpha_D\sim0.095,D_c\sim5.4\times10^{13}
$$

$$
L(C)=(\frac{C_c}{C})^{\alpha_C}, \alpha_C\sim0.050,C_c\sim3.1\times10^{8}
$$

其中L表示不考虑另外两个因素时单个因素的交叉熵损失函数。交叉熵（真实标签与网络预测结果的差异）。预测分布越接近真实分布，交叉熵损失越小，预测分布越远离真实分布，交叉熵损失越大。这三个函数通过在不同参数下训练模型并拟合模型性能得到。模型性能与这三个规模因素强相关，而与模型shape（自注意力头数、深度等）弱相关。

- **Chinchilla scaling law.**  作为另一项具有代表性的研究是Google DeepMind团队2022年提出的scaling law的替代形式。他们通过更大范围地改变模型大小(70M~16B)和数据大小(5B~500B)进行试验，并拟合了类似的定律：

![Untitled](A%20Survey%20of%20Large%20Language%20Models%208c4477b6a882437babbaf26fcd835e98/Untitled.png)

其中 $E=1.69,A=406.4,B=410.7,\alpha=0.34,\beta=0.28$ 。在 $C\approx6ND$ 的约束条件下，对损失函数L进行优化，它们发现计算预算对模型大小和数据大小的最优分配可以如下推导：

![Untitled](A%20Survey%20of%20Large%20Language%20Models%208c4477b6a882437babbaf26fcd835e98/Untitled%201.png)

其中 ,$a=\frac{\alpha}{\alpha+\beta},b=\frac{\beta}{\alpha+\beta}$ 。G 是一个比例系数，可以通过 $A,B,\alpha,\beta$ 计算。

在计算预算增加的情况下，KM缩放定律倾向于给模型更大的预算分配。而Chinchilla缩放定律认为，这两个大小应该以相等的比例增加，比例值与a和b的比例相似。

尽管有一些有限的假设，但这些缩放定律提供了对缩放效果的直观理解，使预测LLM在训练期间的性能变得可行。然而模型的部分能力仍然无法预测，如上下文学习能力，只有当模型大小超过一定水平时才能观察到。

## LLM Emergent Ability（涌现能力）

凝聚态物理里面常用涌现一词（英文emergent）来描述随着粒子数目增多突然出现的奇异现象。

在LLM中表现出了小型模型中不具备的能力，这是将LLM与以前的PLM区分开来的最显著的特征之一。不放出现的特殊能力与某些复杂任务相关，而更值得关注的时那些可以应用于解决各种任务的一般能力。

- **In-context learning (ICL).** 上下文学习能力在GPT-3中正式引入。假设现在有一个新的任务，只需要给语言模型提供任务描述或几个示例，它可以产生预期的输出，而不需要额外的训练和梯度更新。175B的GPT-3表现出了很强的上下文学习能力，而GPT-1和GPT-2则没有。这种能力的一个具体表现是模型能够实现3位数加减法，但在部分任务（如波斯语问答）中没有表现出来。
- **Instruction following.** 指令服从是指，通过混合任务数据集进行训练的LLM能够顺利完成以指令方式表述的新任务。使用者输入的指令能够对LLM的结果进行微调，即使模型没有先例可以遵循，其仍然能够根据指令完成新任务。在LaMDA-PT(2022)进行的实验中，当模型大小达到68B，指令微调开始在模型效果中发挥显著作用。而另一项实验中，PaLM至少需要62B才能在benchmark的各类任务中表现良好。当然针对某些任务，可能更小的模型也能够胜任。
- **Step-by-step reasoning.** 小型的语言模型通常很难解决需要多个推理步骤的复杂任务，如数学类相关的问题。GPT-3通过引入prompting机制进行few-shot learning。基于这种想法Wei等人进一步提出chain-of-thought (CoT)策略，把带有中间步骤的样例加入到prompt中，使LLM学习生成自然语言形式的中间推理步骤直到产生最终答案的能力。当模型规模大于60B时，CoT能够带来性能提升，大于100B时，CoT性能显著高于常规的prompting策略。

# GPT发展

## GPT-1

使用Transformer的decoder结构，结合无监督预训练和有监督微调，GPT-1为GPT系列模型建立了核心架构，并确立了对自然语言文本建模的基本原则，即预测下一个单词。

## GPT-2

与GPT-1的架构类似，GPT-2将参数规模增加到1.5B，并使用大型网页数据集WebText进行训练。GPT-2可以直接使用基于无监督训练的模型执行任务，而无需微调。为了使模型具备执行多个任务的能力，GPT-2引入了一种用于多任务求解的概率形式——$p(output|input,task)$。任务，输入，输出可以直接以自然语言序列的形式进行表示，如翻译任务为 (translateto french, english text, french text)。此时解决任务的过程被转换为生成解决文本的单词预测问题。

GPT-2的主张是——“由于（特定任务的）监督与无监督（语言建模）目标相同，但仅在序列的子集上进行评估，因此无监督目标的全局最小值也是监督目标（针对各种任务）的全局最小”。对这一主张的基本理解是，每个（NLP）任务都可以被视为基于世界文本子集的单词预测问题。因此，如果无监督语言建模被训练成具有足够的恢复世界文本的能力，那么它就能够解决各种任务。

## GPT-3

GPT-3首次引入上下文学习的能力，它能够以少样本或零样本的形式使用LLM。ICL帮助LLM以自然语言文本的形式理解任务，将LLM的预训练和使用融合为同一个语言模型范式：预训练基于上下文预测接下来的文本序列，而ICL预测正确的任务解决方案，同样是一种文本序列。