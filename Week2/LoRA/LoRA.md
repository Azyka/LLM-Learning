# LoRA

Adapter和Prompting是LLM中常用的轻量级微调方案。

Low-Rank Adaptation提出不改变预训练模型权重，而在模型外添加旁路，用训练旁路中的矩阵模拟微调过程。这种方法的优点是不引入额外延迟，只需要将旁路参数与模型参数合并，用新参数推理结果即可。

![image-20230607120849961](./images/LoRA/image-20230607120849961.png)

# 背景——微调原理

在2020年，一项由[Aghajanyan等人](https://arxiv.org/abs/2012.13255)进行的研究揭示了微调背后的原理。

fine-tune（微调）：基于规模较小的标注数据集，通过梯度下降等算法调整模型权重，将具有大量参数的预训练模型调整到指定任务上的过程

intrinsic dimensionality（本征维度）：目标函数达到精确解决优化问题所需的最小维度。

对预训练模型而言，衡量本征维度告诉我们在对每个任务进行微调时需要多少空余参数才能大概解决优化问题。

标准的预训练模型仅需少量参数就可以学习大量NLP任务，预训练本身实在为后续NLP任务调整最小化本征维度。因此，文章认为预训练实际上是一个学习如何压缩平均NLP任务的框架。



## 计算本征维度

计算目标函数的精确本征维度非常困难，可以使用启发式方法计算上界。



# 介绍

现有方法缺点

- 完全更新模型参数：参数量过大，效率低下，存储所有微调后的模型相当困难
- Adapter：引入推理延迟，需要在效率和模型质量之间权衡
- prefix tuning（对prompt微调）：优化难度大，其性能在可训练参数中非单调变化。更重要的是，只保留一部分序列长度必然会导致下游任务可用的序列长度减少，可能导致模型性能不如其他方法。



Inspiration：根据背景中描述的研究，学习后的过参数化模型实际上处于一个低“intrinsic dimension”上，作者假设模型adaptation（和fine-tune类似，将模型迁移到一个指定任务上）过程中的权重变化矩阵同样有一个较低的内在秩。

提出方法：通过优化密集层在适应过程中的变化的秩分解矩阵来间接地训练神经网络中的一些密集层，而保持预先训练的权重冻结。



# 方法

神经网络包含许多密集层进行矩阵乘积。这些层中的权重矩阵通常具有全秩。当模型适应一个特定的任务时，研究表明预训练模型具有一个低“内在维度“，尽管被随机投影到较小的子空间，模型仍然可以有效学习。

因此，LoRA假设对模型权重的更新同样有一个低秩。现有预选连权重矩阵   $W_0\in\mathbb{R}^{d\times k}$ ，将权重更新表示为 $W_0+\bigtriangleup W=W_0+BA$，其中 $B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times k}$，秩 $r\ll min(d,k)$。在训练中，与训练权重矩阵保持不变，不接受梯度更新，而将$A,B$作为训练参数矩阵。

对于原来的推理变换 $h=W_0x$，LoRA定义声明为：

 $h=W_0x+\bigtriangleup Wx=W_0+BAx$

使用随即高斯函数初始化A，使用0初始化B，因此$\bigtriangleup W$在训练开始时为0。



# 应用

在Transformer结构中，自注意力模块有4个权重矩阵 $W_q,W_k,W_v,W_o$，MLP（多层感知器，一种前向神经网络）模块有2个权重矩阵。LoRA只微调 $W_q,W_k,W_v$ 中的一个，相当于为下游任务调整时只变更注意力权重，而不变化MLP模块。
